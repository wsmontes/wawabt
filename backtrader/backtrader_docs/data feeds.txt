Skip to content
logo
Backtrader
Data Feeds
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Data Feeds Common parameters
CSV Data Feeds Common parameters
GenericCSVData
Data Feeds
backtrader comes with a set of Data Feed parsers (at the time of writing all CSV Based) to let you load data from different sources.

Yahoo (online or already saved to a file)

VisualChart (see www.visualchart.com

Backtrader CSV (own cooked format for testing)

Generic CSV support

From the Quickstart guide it should be clear that you add data feeds to a Cerebro instance. The data feeds will later be available to the different strategies in:

An array self.datas (insertion order)

Alias to the array objects:

self.data and self.data0 point to the first element

self.dataX points to elements with index X in the array

A quick reminder as to how the insertion works:


import backtrader as bt
import backtrader.feeds as btfeeds

data = btfeeds.YahooFinanceCSVData(dataname='wheremydatacsvis.csv')

cerebro = bt.Cerebro()

cerebro.adddata(data)  # a 'name' parameter can be passed for plotting purposes
Data Feeds Common parameters
This data feed can download data directly from Yahoo and feed into the system.

Parameters:

dataname (default: None) MUST BE PROVIDED

The meaning varies with the data feed type (file location, ticker, â€¦)

name (default: â€˜â€™)

Meant for decorative purposes in plotting. If not specified it may be derived from dataname (example: last part of a file path)

fromdate (default: mindate)

Python datetime object indicating that any datetime prior to this should be ignored

todate (default: maxdate)

Python datetime object indicating that any datetime posterior to this should be ignored

timeframe (default: TimeFrame.Days)

Potential values: Ticks, Seconds, Minutes, Days, Weeks, Months and Years

compression (default: 1)

Number of actual bars per bar. Informative. Only effective in Data Resampling/Replaying.

sessionstart (default: None)

Indication of session starting time for the data. May be used by classes for purposes like resampling

sessionend (default: None)

Indication of session ending time for the data. May be used by classes for purposes like resampling

CSV Data Feeds Common parameters
Parameters (additional to the common ones):

headers (default: True)

Indicates if the passed data has an initial headers row

separator (default: â€œ,â€)

Separator to take into account to tokenize each of the CSV rows

GenericCSVData
This class exposes a generic interface allowing parsing mostly every CSV file format out there.

Parses a CSV file according to the order and field presence defined by the parameters

Specific parameters (or specific meaning):

dataname

The filename to parse or a file-like object

datetime (default: 0) column containing the date (or datetime) field

time (default: -1) column containing the time field if separate from the datetime field (-1 indicates itâ€™s not present)

open (default: 1) , high (default: 2), low (default: 3), close (default: 4), volume (default: 5), openinterest (default: 6)

Index of the columns containing the corresponding fields

If a negative value is passed (example: -1) it indicates the field is not present in the CSV data

nullvalue (default: float(â€˜NaNâ€™))

Value that will be used if a value which should be there is missing (the CSV field is empty)

dtformat (default: %Y-%m-%d %H:%M:%S)

Format used to parse the datetime CSV field

tmformat (default: %H:%M:%S)

Format used to parse the time CSV field if â€œpresentâ€ (the default for the â€œtimeâ€ CSV field is not to be present)

An example usage covering the following requirements:

Limit input to year 2000

HLOC order rather than OHLC

Missing values to be replaced with zero (0.0)

Daily bars are provided and datetime is just the day with format YYYY-MM-DD

No openinterest column is present

The code:


import datetime
import backtrader as bt
import backtrader.feeds as btfeeds

...
...

data = btfeeds.GenericCSVData(
    dataname='mydata.csv',

    fromdate=datetime.datetime(2000, 1, 1),
    todate=datetime.datetime(2000, 12, 31),

    nullvalue=0.0,

    dtformat=('%Y-%m-%d'),

    datetime=0,
    high=1,
    low=2,
    open=3,
    close=4,
    volume=5,
    openinterest=-1
)

...
Slightly modified requirements:

Limit input to year 2000

HLOC order rather than OHLC

Missing values to be replaced with zero (0.0)

Intraday bars are provided, with separate date and time columns

Date has format YYYY-MM-DD
Time has format HH.MM.SS (instead of the usual HH:MM:SS)
No openinterest column is present

The code:


import datetime
import backtrader as bt
import backtrader.feeds as btfeed

...
...

data = btfeeds.GenericCSVData(
    dataname='mydata.csv',

    fromdate=datetime.datetime(2000, 1, 1),
    todate=datetime.datetime(2000, 12, 31),

    nullvalue=0.0,

    dtformat=('%Y-%m-%d'),
    tmformat=('%H.%M.%S'),

    datetime=0,
    time=1,
    high=2,
    low=3,
    open=4,
    close=5,
    volume=6,
    openinterest=-1
)
This can also be made permanent with subclassing:


import datetime
import backtrader.feeds as btfeed

class MyHLOC(btfreeds.GenericCSVData):

  params = (
    ('fromdate', datetime.datetime(2000, 1, 1)),
    ('todate', datetime.datetime(2000, 12, 31)),
    ('nullvalue', 0.0),
    ('dtformat', ('%Y-%m-%d')),
    ('tmformat', ('%H.%M.%S')),

    ('datetime', 0),
    ('time', 1),
    ('high', 2),
    ('low', 3),
    ('open', 4),
    ('close', 5),
    ('volume', 6),
    ('openinterest', -1)
)
This new class can be reused now by just providing the dataname:


data = btfeeds.MyHLOC(dataname='mydata.csv')
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Extending
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Plotting that extra P/E line
Extending a Datafeed
Issues in GitHub are actually pushing into finishing documentation parts or helping me to understand if backtrader has the ease of use and flexibility I envisioned from the first moments and the decisions made along the way.

In this case is Issue #9.

The question finally seems to boil down to:

Can the end user easily extend the existing mechanisms to add extra information in the form of lines that gets passed along other existing price information spots like open, high, etc?
As far as I understand the question the answer is: Yes

The poster seems to have these requirements (from Issue #6):

A data source which is being parsed into CSV format

Using GenericCSVData to load the information

This generic csv support was developed in response to this Issue #6

An extra field which apparently contains P/E information which needs to be passed along the parsed CSV Data

Letâ€™s build on the CSV Data Feed Development and GenericCSVData example posts.

Steps:

Assume the P/E information is being set in the CSV data which is parsed

Use GenericCSVData as the base class

Extend the existng lines (open/high/low/close/volumen/openinterest) with pe

Add a parameter to let the caller determine the column position of the P/E information

The result:


from backtrader.feeds import GenericCSVData

class GenericCSV_PE(GenericCSVData):

    # Add a 'pe' line to the inherited ones from the base class
    lines = ('pe',)

    # openinterest in GenericCSVData has index 7 ... add 1
    # add the parameter to the parameters inherited from the base class
    params = (('pe', 8),)
And the job is done â€¦

Later and when using this data feed inside a strategy:


import backtrader as bt

....

class MyStrategy(bt.Strategy):

    ...

    def next(self):

        if self.data.close > 2000 and self.data.pe < 12:
            # TORA TORA TORA --- Get off this market
            self.sell(stake=1000000, price=0.01, exectype=Order.Limit)
    ...
Plotting that extra P/E line
There is obviously no automated plot support for that extra line in the data feed.

The best alternative would be to do a SimpleMovingAverage on that line and plot it in a separate axis:


import backtrader as bt
import backtrader.indicators as btind

....

class MyStrategy(bt.Strategy):

    def __init__(self):

        # The indicator autoregisters and will plot even if no obvious
        # reference is kept to it in the class
        btind.SMA(self.data.pe, period=1, subplot=False)

    ...

    def next(self):

        if self.data.close > 2000 and self.data.pe < 12:
            # TORA TORA TORA --- Get off this market
            self.sell(stake=1000000, price=0.01, exectype=Order.Limit)
    ...
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Development - CSV
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Caveat Emptor
CSV Data Feed Development
backtrader already offers a Generic CSV Data feed and some specific CSV Data Feeds. Summarizing:

GenericCSVData

VisualChartCSVData

YahooFinanceData (for online downloads)

YahooFinanceCSVData (for already downloaded data)

BacktraderCSVData (in-house â€¦ for testing purposed, but can be used)

But even with that, the end user may wish to develop support for a specific CSV Data Feed.

The usual motto would be: â€œItâ€™s easier said than doneâ€. Actually the structure is meant to make it easy.

Steps:

Inherit from backtrader.CSVDataBase

Define any params if needed

Do any initialization in the start method

Do any clean-up in the stop method

Define a _loadline method where the actual work happens

This method receives a single argument: linetokens.

As the name suggests this contains the tokens after the current line has been splitten according to the separator parameter (inherited from the base class)

If after doing its work there is new data â€¦ fill up the corresponding lines and return True

If nothing is available and therefore the parsing has come to an end: return False

Returning False may not even be needed if the behind the scenes code which is reading the file lines finds out there are no more lines to parse.

Things which are already taken into account:

Opening the file (or receiving a file-like object)

Skipping the headers row if indicated as present

Reading the lines

Tokenizing the lines

Preloading support (to load the entire data feed at once in memory)

Usually an example is worth a thousand requirement descriptions. Letâ€™s use a simplified version of the in-house defined CSV parsing code from BacktraderCSVData. This one needs no initialization or clean-up (this could be opening a socket and closing it later, for example).

Note

backtrader data feeds contain the usual industry standard feeds, which are the ones to be filled. Namely:

datetime

open

high

low

close

volume

openinterest

If your strategy/algorithm or simple data perusal only needs, for example the closing prices you can leave the others untouched (each iteration fills them automatically with a float(â€˜NaNâ€™) value before the end user code has a chance to do anything.

In this example only a daily format is supported:


import itertools
...
import backtrader as bt

class MyCSVData(bt.CSVDataBase):

    def start(self):
        # Nothing to do for this data feed type
        pass

    def stop(self):
        # Nothing to do for this data feed type
        pass

    def _loadline(self, linetokens):
        i = itertools.count(0)

        dttxt = linetokens[next(i)]
        # Format is YYYY-MM-DD
        y = int(dttxt[0:4])
        m = int(dttxt[5:7])
        d = int(dttxt[8:10])

        dt = datetime.datetime(y, m, d)
        dtnum = date2num(dt)

        self.lines.datetime[0] = dtnum
        self.lines.open[0] = float(linetokens[next(i)])
        self.lines.high[0] = float(linetokens[next(i)])
        self.lines.low[0] = float(linetokens[next(i)])
        self.lines.close[0] = float(linetokens[next(i)])
        self.lines.volume[0] = float(linetokens[next(i)])
        self.lines.openinterest[0] = float(linetokens[next(i)])

        return True
The code expects all fields to be in place and be convertible to floats, except for the datetime which has a fixed YYYY-MM-DD format and can be parsed without using datetime.datetime.strptime.

More complex needs can be covered by adding just a few lines of code to account for null values, date format parsing. The GenericCSVData does that.

Caveat Emptor
Using the GenericCSVData existing feed and inheritance a lot can be acomplished in order to support formats.

Letâ€™s add support for Sierra Chart daily format (which is always stored in CSV format).

Definition (by looking into one of the â€˜.dlyâ€™ data files:

Fields: Date, Open, High, Low, Close, Volume, OpenInterest

The industry standard ones and the ones already supported by GenericCSVData in the same order (which is also industry standard)

Separator: ,

Date Format: YYYY/MM/DD

A parser for those files:


class SierraChartCSVData(backtrader.feeds.GenericCSVData):

    params = (('dtformat', '%Y/%m/%d'),)
The params definition simply redefines one of the existing parameters in the base class. In this case just the formatting string for dates needs a change.

Et voilÃ¡ â€¦ the parser for Sierra Chart is finished.

Here below the parameters definition of GenericCSVData as a reminder:


class GenericCSVData(feed.CSVDataBase):
    params = (
        ('nullvalue', float('NaN')),
        ('dtformat', '%Y-%m-%d %H:%M:%S'),
        ('tmformat', '%H:%M:%S'),

        ('datetime', 0),
        ('time', -1),
        ('open', 1),
        ('high', 2),
        ('low', 3),
        ('close', 4),
        ('volume', 5),
        ('openinterest', 6),
    )
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Development - General
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Sample binary datafeed
Initialization
Start
Stop
Actual Loading
Other Binary Formats
VChartData Test
VChartData Full Code
Binary Datafeed Development
Note

The binary file used in the examples goog.fd belongs to VisualChart and cannot be distributed with backtrader.

VisualChart can be downloaded free of charge for those interested in directly using the binary files.

CSV Data feed development has shown how to add new CSV based data feeds. The existing base class CSVDataBase provides the framework taking most of the work off the subclasses which in most cases can simply do:


def _loadline(self, linetokens):

  # parse the linetokens here and put them in self.lines.close,
  # self.lines.high, etc

  return True # if data was parsed, else ... return False
The base class takes care of the parameters, initialization, opening of files, reading lines, splitting the lines in tokens and additional things like skipping lines which donâ€™t fit into the date range (fromdate, todate) which the end user may have defined.

Developing a non-CSV datafeed follows the same pattern without going down to the already splitted line tokens.

Things to do:

Derive from backtrader.feed.DataBase

Add any parameters you may need

Should initialization be needed, override __init__(self) and/or start(self)

Should any clean-up code be needed, override stop(self)

The work happens inside the method which MUST always be overriden: _load(self)

Letâ€™s the parameters already provided by backtrader.feed.DataBase:


from backtrader.utils.py3 import with_metaclass

...
...

class DataBase(with_metaclass(MetaDataBase, dataseries.OHLCDateTime)):

    params = (('dataname', None),
        ('fromdate', datetime.datetime.min),
        ('todate', datetime.datetime.max),
        ('name', ''),
        ('compression', 1),
        ('timeframe', TimeFrame.Days),
        ('sessionend', None))
Having the following meanings:

dataname is what allows the data feed to identify how to fetch the data. In the case of the CSVDataBase this parameter is meant to be a path to a file or already a file-like object.

fromdate and todate define the date range which will be passed to strategies. Any value provided by the feed outside of this range will be ignored

name is cosmetic for plotting purposes

timeframe indicates the temporal working reference

Potential values: Ticks, Seconds, Minutes, Days, Weeks, Months and Years

compression (default: 1)

Number of actual bars per bar. Informative. Only effective in Data Resampling/Replaying.

compression

sessionend if passed (a datetime.time object) will be added to the datafeed datetime line which allows identifying the end of the session

Sample binary datafeed
backtrader already defines a CSV datafeed (VChartCSVData) for the exports of VisualChart, but it is also possible to directly read the binary data files.

Letâ€™s do it (full data feed code can be found at the bottom)

Initialization
The binary VisualChart data files can contain either daily (.fd extension) or intraday data (.min extension). Here the parameter timeframe will be used to distinguish which type of file is being read.

During __init__ constants which differ for each type are set up.


    def __init__(self):
        super(VChartData, self).__init__()

        # Use the informative "timeframe" parameter to understand if the
        # code passed as "dataname" refers to an intraday or daily feed
        if self.p.timeframe >= TimeFrame.Days:
            self.barsize = 28
            self.dtsize = 1
            self.barfmt = 'IffffII'
        else:
            self.dtsize = 2
            self.barsize = 32
            self.barfmt = 'IIffffII'
Start
The Datafeed will be started when backtesting commences (it can actually be started several times during optimizations)

In the start method the binary file is open unless a file-like object has been passed.


    def start(self):
        # the feed must start ... get the file open (or see if it was open)
        self.f = None
        if hasattr(self.p.dataname, 'read'):
            # A file has been passed in (ex: from a GUI)
            self.f = self.p.dataname
        else:
            # Let an exception propagate
            self.f = open(self.p.dataname, 'rb')
Stop
Called when backtesting is finished.

If a file was open, it will be closed


    def stop(self):
        # Close the file if any
        if self.f is not None:
            self.f.close()
            self.f = None
Actual Loading
The actual work is done in _load. Called to load the next set of data, in this case the next : datetime, open, high, low, close, volume, openinterest. In backtrader the â€œactualâ€ moment corresponds to index 0.

A number of bytes will be read from the open file (determined by the constants set up during __init__), parsed with the struct module, further processed if needed (like with divmod operations for date and time) and stored in the lines of the data feed: datetime, open, high, low, close, volume, openinterest.

If no data can be read from the file it is assumed that the End Of File (EOF) has been reached

False is returned to indicate the fact no more data is available
Else if data has been loaded and parsed:

True is returned to indicate the loading of the data set was a success

    def _load(self):
        if self.f is None:
            # if no file ... no parsing
            return False

        # Read the needed amount of binary data
        bardata = self.f.read(self.barsize)
        if not bardata:
            # if no data was read ... game over say "False"
            return False

        # use struct to unpack the data
        bdata = struct.unpack(self.barfmt, bardata)

        # Years are stored as if they had 500 days
        y, md = divmod(bdata[0], 500)
        # Months are stored as if they had 32 days
        m, d = divmod(md, 32)
        # put y, m, d in a datetime
        dt = datetime.datetime(y, m, d)

        if self.dtsize > 1:  # Minute Bars
            # Daily Time is stored in seconds
            hhmm, ss = divmod(bdata[1], 60)
            hh, mm = divmod(hhmm, 60)
            # add the time to the existing atetime
            dt = dt.replace(hour=hh, minute=mm, second=ss)

        self.lines.datetime[0] = date2num(dt)

        # Get the rest of the unpacked data
        o, h, l, c, v, oi = bdata[self.dtsize:]
        self.lines.open[0] = o
        self.lines.high[0] = h
        self.lines.low[0] = l
        self.lines.close[0] = c
        self.lines.volume[0] = v
        self.lines.openinterest[0] = oi

        # Say success
        return True
Other Binary Formats
The same model can be applied to any other binary source:

Database

Hierarchical data storage

Online source

The steps again:

__init__ -> Any init code for the instance, only once

start -> start of backtesting (one or more times if optimization will be run)

This would for example open the connection to the database or a socket to an online service

stop -> clean-up like closing the database connection or open sockets

_load -> query the database or online source for the next set of data and load it into the lines of the object. The standard fields being: datetime, open, high, low, close, volume, openinterest

VChartData Test
The VCharData loading data from a local â€œ.fdâ€ file for Google for the year 2006.

Itâ€™s only about loading the data, so not even a subclass of Strategy is needed.


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import datetime

import backtrader as bt
from vchart import VChartData


if __name__ == '__main__':
    # Create a cerebro entity
    cerebro = bt.Cerebro(stdstats=False)

    # Add a strategy
    cerebro.addstrategy(bt.Strategy)

    ###########################################################################
    # Note:
    # The goog.fd file belongs to VisualChart and cannot be distributed with
    # backtrader
    #
    # VisualChart can be downloaded from www.visualchart.com
    ###########################################################################
    # Create a Data Feed
    datapath = '../../datas/goog.fd'
    data = VChartData(
        dataname=datapath,
        fromdate=datetime.datetime(2006, 1, 1),
        todate=datetime.datetime(2006, 12, 31),
        timeframe=bt.TimeFrame.Days
    )

    # Add the Data Feed to Cerebro
    cerebro.adddata(data)

    # Run over everything
    cerebro.run()

    # Plot the result
    cerebro.plot(style='bar')
image

VChartData Full Code

from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import datetime
import struct

from backtrader.feed import DataBase
from backtrader import date2num
from backtrader import TimeFrame


class VChartData(DataBase):
    def __init__(self):
        super(VChartData, self).__init__()

        # Use the informative "timeframe" parameter to understand if the
        # code passed as "dataname" refers to an intraday or daily feed
        if self.p.timeframe >= TimeFrame.Days:
            self.barsize = 28
            self.dtsize = 1
            self.barfmt = 'IffffII'
        else:
            self.dtsize = 2
            self.barsize = 32
            self.barfmt = 'IIffffII'

    def start(self):
        # the feed must start ... get the file open (or see if it was open)
        self.f = None
        if hasattr(self.p.dataname, 'read'):
            # A file has been passed in (ex: from a GUI)
            self.f = self.p.dataname
        else:
            # Let an exception propagate
            self.f = open(self.p.dataname, 'rb')

    def stop(self):
        # Close the file if any
        if self.f is not None:
            self.f.close()
            self.f = None

    def _load(self):
        if self.f is None:
            # if no file ... no parsing
            return False

        # Read the needed amount of binary data
        bardata = self.f.read(self.barsize)
        if not bardata:
            # if no data was read ... game over say "False"
            return False

        # use struct to unpack the data
        bdata = struct.unpack(self.barfmt, bardata)

        # Years are stored as if they had 500 days
        y, md = divmod(bdata[0], 500)
        # Months are stored as if they had 32 days
        m, d = divmod(md, 32)
        # put y, m, d in a datetime
        dt = datetime.datetime(y, m, d)

        if self.dtsize > 1:  # Minute Bars
            # Daily Time is stored in seconds
            hhmm, ss = divmod(bdata[1], 60)
            hh, mm = divmod(hhmm, 60)
            # add the time to the existing atetime
            dt = dt.replace(hour=hh, minute=mm, second=ss)

        self.lines.datetime[0] = date2num(dt)

        # Get the rest of the unpacked data
        o, h, l, c, v, oi = bdata[self.dtsize:]
        self.lines.open[0] = o
        self.lines.high[0] = h
        self.lines.low[0] = l
        self.lines.close[0] = c
        self.lines.volume[0] = v
        self.lines.openinterest[0] = oi

        # Say success
        return True
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Multiple Timeframes
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Example 1 - Daily and Weekly
Example 2 - Daily and Daily Compression (2 bars to 1)
Example 3 - Strategy with SMA
Invocation 1:
Invocation 2:
Conclusion
Data - Multiple Timeframes
Sometimes investing decisions are taken using different timeframes:

Weekly to evaluate the trend

Daily to execute the entry

Or 5 minutes vs 60 minutes.

That implies that combining datas of multiple timeframes in backtrader is needed to support such combinations.

Native support for it is already built-in. The end user must only follow these rules:

The data with the smallest timeframe (and thus the larger number of bars) must be the 1st one to be added to the Cerebro instance

The datas must be properly date-time aligned for the platform to make any sense out of them

Beyond that, the end-user is free to apply indicators as wished on the shorter/larger timeframes. Of course:

Indicators applied to larger timeframes will produce less bars
The platform will also have the following into account

The minimum period for larger timeframes
Minimum period which will probably have the side effect of having to consume several orders of magnitude of the smaller timeframe bars before a Strategy added to Cerebro kicks into action.

The built-in cerebro.resample is going to be used to create a larger timeframe.

Some examples below, but first the sauce of the test script.


    # Load the Data
    datapath = args.dataname or '../../datas/2006-day-001.txt'
    data = btfeeds.BacktraderCSVData(dataname=datapath)
    cerebro.adddata(data)  # First add the original data - smaller timeframe

    tframes = dict(daily=bt.TimeFrame.Days, weekly=bt.TimeFrame.Weeks,
                   monthly=bt.TimeFrame.Months)

    # Handy dictionary for the argument timeframe conversion
    # Resample the data
    if args.noresample:
        datapath = args.dataname2 or '../../datas/2006-week-001.txt'
        data2 = btfeeds.BacktraderCSVData(dataname=datapath)
        # And then the large timeframe
        cerebro.adddata(data2)
    else:
        cerebro.resampledata(data, timeframe=tframes[args.timeframe],
                             compression=args.compression)

    # Run over everything
    cerebro.run()
The steps:

Load a data

Resample it according to the user specified arguments

The script also allows for loading a 2nd data

Add the data to cerebro

Add the resampled data (larger timeframe) to cerebro

run

Example 1 - Daily and Weekly
The invocation of the script:


$ ./multitimeframe-example.py --timeframe weekly --compression 1
And the output chart:

image

Example 2 - Daily and Daily Compression (2 bars to 1)
The invocation of the script:


$ ./multitimeframe-example.py --timeframe daily --compression 2
And the output chart:

image

Example 3 - Strategy with SMA
Although plotting is nice, the key issue here is showing how the larger timeframe influences the system, especially when it comes down to the starting point

The script can take a --indicators to add a strategy which creates simple moving averages of period 10 on the smaller an larger timeframe datas.

If only the smaller timeframe was taken into account:

next would be called first after 10 bars, which is the time the Simple Moving Average needs to produce a value

NOTE: Remember that Strategy monitors created indicators and only calls next when all indicators have produced a value. The rationale is that the end user has added the indicators to use them in the logic and thus no logic should take place if the indicators have produced no values

But in this case the larger timeframe (weekly) delays the invocation of next until the Simple Moving Average oon the weekly data has produced a value, which takes â€¦ 10 weeks.

The script overrides nextstart which is only called once and which defaults to calling next to show when it is first called.

Invocation 1:
Only the smaller timeframe, daily, gets a Simple Moving Average

The command line and output


$ ./multitimeframe-example.py --timeframe weekly --compression 1 --indicators --onlydaily
--------------------------------------------------
nextstart called with len 10
--------------------------------------------------
And the chart.

image

Invocation 2:
Both timeframes get a Simple Moving Average

The command line:


$ ./multitimeframe-example.py --timeframe weekly --compression 1 --indicators
--------------------------------------------------
nextstart called with len 50
--------------------------------------------------
--------------------------------------------------
nextstart called with len 51
--------------------------------------------------
--------------------------------------------------
nextstart called with len 52
--------------------------------------------------
--------------------------------------------------
nextstart called with len 53
--------------------------------------------------
--------------------------------------------------
nextstart called with len 54
--------------------------------------------------
Two things to notice here:

Instead of being called after 10 periods, the strategy is 1st called after 50 periods.

It is so because the Simple Moving Average applied on the larger (weekly) timeframe produces a value after 10 weeks â€¦ and that is 10 weeks * 5 days / week â€¦ 50 days

nextstart gets called 5 times rather than only 1.

This is a natural side effect of having mixed the timeframe and having (in this case only one) indicators applied to the larger timeframe.

The larger timeframe Simple Moving Average produces 5 times the same value whilst 5 daily bars are being consumed.

And because the start of the period is being controlled by the larger timeframe nextstart gets called 5 times.

And the chart.

image

Conclusion
Multiple Timeframe Datas can be used in backtrader with no special objects or tweaking: just add the smaller timeframes first.

The test script.


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import argparse

import backtrader as bt
import backtrader.feeds as btfeeds
import backtrader.indicators as btind


class SMAStrategy(bt.Strategy):
    params = (
        ('period', 10),
        ('onlydaily', False),
    )

    def __init__(self):
        self.sma_small_tf = btind.SMA(self.data, period=self.p.period)
        if not self.p.onlydaily:
            self.sma_large_tf = btind.SMA(self.data1, period=self.p.period)

    def nextstart(self):
        print('--------------------------------------------------')
        print('nextstart called with len', len(self))
        print('--------------------------------------------------')

        super(SMAStrategy, self).nextstart()


def runstrat():
    args = parse_args()

    # Create a cerebro entity
    cerebro = bt.Cerebro(stdstats=False)

    # Add a strategy
    if not args.indicators:
        cerebro.addstrategy(bt.Strategy)
    else:
        cerebro.addstrategy(
            SMAStrategy,

            # args for the strategy
            period=args.period,
            onlydaily=args.onlydaily,
        )

    # Load the Data
    datapath = args.dataname or '../../datas/2006-day-001.txt'
    data = btfeeds.BacktraderCSVData(dataname=datapath)
    cerebro.adddata(data)  # First add the original data - smaller timeframe

    tframes = dict(daily=bt.TimeFrame.Days, weekly=bt.TimeFrame.Weeks,
                   monthly=bt.TimeFrame.Months)

    # Handy dictionary for the argument timeframe conversion
    # Resample the data
    if args.noresample:
        datapath = args.dataname2 or '../../datas/2006-week-001.txt'
        data2 = btfeeds.BacktraderCSVData(dataname=datapath)
        # And then the large timeframe
        cerebro.adddata(data2)
    else:
        cerebro.resampledata(data, timeframe=tframes[args.timeframe],
                             compression=args.compression)

    # Run over everything
    cerebro.run()

    # Plot the result
    cerebro.plot(style='bar')


def parse_args():
    parser = argparse.ArgumentParser(
        description='Multitimeframe test')

    parser.add_argument('--dataname', default='', required=False,
                        help='File Data to Load')

    parser.add_argument('--dataname2', default='', required=False,
                        help='Larger timeframe file to load')

    parser.add_argument('--noresample', action='store_true',
                        help='Do not resample, rather load larger timeframe')

    parser.add_argument('--timeframe', default='weekly', required=False,
                        choices=['daily', 'weekly', 'monhtly'],
                        help='Timeframe to resample to')

    parser.add_argument('--compression', default=1, required=False, type=int,
                        help='Compress n bars into 1')

    parser.add_argument('--indicators', action='store_true',
                        help='Wether to apply Strategy with indicators')

    parser.add_argument('--onlydaily', action='store_true',
                        help='Indicator only to be applied to daily timeframe')

    parser.add_argument('--period', default=10, required=False, type=int,
                        help='Period to apply to indicator')

    return parser.parse_args()


if __name__ == '__main__':
    runstrat()
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Resample
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Data Resampling
When data is only available in a single timeframe and the analysis has to be done for a different timeframe, itâ€™s time to do some resampling.

â€œResamplingâ€ should actually be called â€œUpsamplingâ€ given that one goes from a source timeframe to a larger time frame (for example: days to weeks)

backtrader has built-in support for resampling by passing the original data through a filter object. Although there are several ways to achieve this, a straightforward interface exists to achieve this:

Instead of using cerebro.adddata(data) to put a data into the system use

cerebro.resampledata(data, **kwargs)

There are two main options that can be controlled

Change the timeframe

Compress bars

To do so, use the following parameters when calling resampledata:

timeframe (default: bt.TimeFrame.Days)

Destination timeframe which to be useful has to be equal or larger than the source

compression (default: 1)

Compress the selected value â€œnâ€ to 1 bar

Letâ€™s see an example from Daily to Weekly with a handcrafted script:


$ ./resampling-example.py --timeframe weekly --compression 1
The output:

image

We can compare it to the original daily data:


$ ./resampling-example.py --timeframe daily --compression 1
The output:

image

The magic is done by executing the following steps:

Loading the data as usual

Feeding the data into cerebro with resampledata with the desired parameters:

timeframe

compression

The code in the sample (the entire script at the bottom).


    # Load the Data
    datapath = args.dataname or '../../datas/2006-day-001.txt'
    data = btfeeds.BacktraderCSVData(dataname=datapath)

    # Handy dictionary for the argument timeframe conversion
    tframes = dict(
        daily=bt.TimeFrame.Days,
        weekly=bt.TimeFrame.Weeks,
        monthly=bt.TimeFrame.Months)

    # Add the resample data instead of the original
    cerebro.resampledata(data,
                         timeframe=tframes[args.timeframe],
                         compression=args.compression)
A last example in which we first change the time frame from daily to weekly and then apply a 3 to 1 compression:


$ ./resampling-example.py --timeframe weekly --compression 3
The output:

image

From the original 256 daily bars we end up with 18 3-week bars. The breakdown:

52 weeks

52 / 3 = 17.33 and therefore 18 bars

It doesnâ€™t take much more. Of course intraday data can also be resampled.

The resampling filter supports additional parameters, which in most cases should not be touched:

bar2edge (default: True)

resamples using time boundaries as the target. For example with a â€œticks -> 5 secondsâ€ the resulting 5 seconds bars will be aligned to xx:00, xx:05, xx:10 â€¦

adjbartime (default: True)

Use the time at the boundary to adjust the time of the delivered resampled bar instead of the last seen timestamp. If resampling to â€œ5 secondsâ€ the time of the bar will be adjusted for example to hhğŸ‡²ğŸ‡²05 even if the last seen timestamp was hhğŸ‡²ğŸ‡²04.33

Note

Time will only be adjusted if â€œbar2edgeâ€ is True. It wouldnâ€™t make sense to adjust the time if the bar has not been aligned to a boundary

rightedge (default: True)

Use the right edge of the time boundaries to set the time.

If False and compressing to 5 seconds the time of a resampled bar for seconds between hhğŸ‡²ğŸ‡²00 and hhğŸ‡²ğŸ‡²04 will be hhğŸ‡²ğŸ‡²00 (the starting boundary

If True the used boundary for the time will be hhğŸ‡²ğŸ‡²05 (the ending boundary)

boundoff (default: 0)

Push the boundary for resampling/replaying by an amount of units.

If for example the resampling is from 1 minute to 15 minutes, the default behavior is to take the 1-minute bars from 00:01:00 until 00:15:00 to produce a 15-minutes replayed/resampled bar.

If boundoff is set to 1, then the boundary is pushed 1 unit forward. In this case the original unit is a 1-minute bar. Consequently the resampling/replaying will now:

Use the bars from 00:00:00 to 00:14:00 for the generation of the 15-minutes bar
The sample code for the resampling test script.


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import argparse

import backtrader as bt
import backtrader.feeds as btfeeds


def runstrat():
    args = parse_args()

    # Create a cerebro entity
    cerebro = bt.Cerebro(stdstats=False)

    # Add a strategy
    cerebro.addstrategy(bt.Strategy)

    # Load the Data
    datapath = args.dataname or '../../datas/2006-day-001.txt'
    data = btfeeds.BacktraderCSVData(dataname=datapath)

    # Handy dictionary for the argument timeframe conversion
    tframes = dict(
        daily=bt.TimeFrame.Days,
        weekly=bt.TimeFrame.Weeks,
        monthly=bt.TimeFrame.Months)

    # Add the resample data instead of the original
    cerebro.resampledata(data,
                         timeframe=tframes[args.timeframe],
                         compression=args.compression)

    # Run over everything
    cerebro.run()

    # Plot the result
    cerebro.plot(style='bar')


def parse_args():
    parser = argparse.ArgumentParser(
        description='Pandas test script')

    parser.add_argument('--dataname', default='', required=False,
                        help='File Data to Load')

    parser.add_argument('--timeframe', default='weekly', required=False,
                        choices=['daily', 'weekly', 'monhtly'],
                        help='Timeframe to resample to')

    parser.add_argument('--compression', default=1, required=False, type=int,
                        help='Compress n bars into 1')

    return parser.parse_args()


if __name__ == '__main__':
    runstrat()
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Replay
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Example - Replay Daily to Weekly
Example 2 - Daily to Daily with Compression
Conclusion
Data - Replay
The time is gone and testing a strategy against a fully formed and closed bar is good, but it could be better.

This is where Data Replay comes in to help. If:

The strategy operates on data with a timeframe X (example: daily)
and

Data for a smaller timeframe Y (example: 1 minute) is available
Data replay does exactly what the name implies:

Replay a daily bar using the 1 minute data
This is of course not exactly how the market developed, but it is far better than looking at the daily fully formed and closed bar in isolation:

If the strategy operates in realtime during the formation of the daily bar, the approximation of the formation of the bar gives a chance to replicate the actual behavior of the strategy under real conditions

Putting Data Replay into action follows the regular usage patterns of backtrader

Load a data feed

Pass the data to cerebro with replaydata

Add a strategy

Note

Preloading is not supported when data is being replayed because each bar is actually built in real-time. It will automatically disabled in any Cerebro instance.

Parameters which can be passed to replaydata:

timeframe (default: bt.TimeFrame.Days)

Destination timeframe which to be useful has to be equal or larger than the source

compression (default: 1)

Compress the selected value â€œnâ€ to 1 bar

Extended parameters (do not touch if not really needed):

bar2edge (default: True)

replays using time boundaries as the target of the closed bar. For example with a â€œticks -> 5 secondsâ€ the resulting 5 seconds bars will be aligned to xx:00, xx:05, xx:10 â€¦

adjbartime (default: False)

Use the time at the boundary to adjust the time of the delivered resampled bar instead of the last seen timestamp. If resampling to â€œ5 secondsâ€ the time of the bar will be adjusted for example to hhğŸ‡²ğŸ‡²05 even if the last seen timestamp was hhğŸ‡²ğŸ‡²04.33

NOTE: Time will only be adjusted if â€œbar2edgeâ€ is True. It wouldnâ€™t make sense to adjust the time if the bar has not been aligned to a boundary

rightedge (default: True)

Use the right edge of the time boundaries to set the time.

If False and compressing to 5 seconds the time of a resampled bar for seconds between hhğŸ‡²ğŸ‡²00 and hhğŸ‡²ğŸ‡²04 will be hhğŸ‡²ğŸ‡²00 (the starting boundary

If True the used boundary for the time will be hhğŸ‡²ğŸ‡²05 (the ending boundary)

For the sake of working with a example the standard 2006 daily data will be replayed on a weekly basis. Which means:

There will finally be 52 bars, one for each week

Cerebro will call prenext and next a total of 255 times, which is the original count of daily bars

The trick:

When a weekly bar is forming, the length (len(self)) of the strategy will remain unchanged.

With each new week the length will increase by one

Some examples below, but first the sauce of the test script in which the data is loaded and passed to cerebro with replaydata â€¦ and then run.


    # Load the Data
    datapath = args.dataname or '../../datas/2006-day-001.txt'
    data = btfeeds.BacktraderCSVData(dataname=datapath)

    # Handy dictionary for the argument timeframe conversion
    tframes = dict(
        daily=bt.TimeFrame.Days,
        weekly=bt.TimeFrame.Weeks,
        monthly=bt.TimeFrame.Months)

    # First add the original data - smaller timeframe
    cerebro.replaydata(data,
                       timeframe=tframes[args.timeframe],
                       compression=args.compression)
Example - Replay Daily to Weekly
The invocation of the script:


$ ./replay-example.py --timeframe weekly --compression 1
The chart cannot unfortunately show us the real thing happening in the background, so letâ€™s have a look at the console output:


prenext len 1 - counter 1
prenext len 1 - counter 2
prenext len 1 - counter 3
prenext len 1 - counter 4
prenext len 1 - counter 5
prenext len 2 - counter 6
...
...
prenext len 9 - counter 44
prenext len 9 - counter 45
---next len 10 - counter 46
---next len 10 - counter 47
---next len 10 - counter 48
---next len 10 - counter 49
---next len 10 - counter 50
---next len 11 - counter 51
---next len 11 - counter 52
---next len 11 - counter 53
...
...
---next len 51 - counter 248
---next len 51 - counter 249
---next len 51 - counter 250
---next len 51 - counter 251
---next len 51 - counter 252
---next len 52 - counter 253
---next len 52 - counter 254
---next len 52 - counter 255
As we see the internal self.counter variable is keeping track of each call to either prenext or next. The former being called before the applied Simple Moving Average produces a value. The latter called when the Simple Moving Average is producing values.

The key:

The length (len(self)) of the strategy changes every 5 bars (5 trading days in the week)
The strategy is effectively seeing:

How the weekly bar developed in 5 shots.

This, again, doesnâ€™t replicate the actual tick-by-tick (and not even minute, hour) development of the market, but it is better than actually seeing a bar.

The visual output is that of the weekly chart which is the final outcome the system is being tested again.

image

Example 2 - Daily to Daily with Compression
Of course â€œReplayingâ€ can be applied to the same timeframe but with a compression.

The console:


$ ./replay-example.py --timeframe daily --compression 2
prenext len 1 - counter 1
prenext len 1 - counter 2
prenext len 2 - counter 3
prenext len 2 - counter 4
prenext len 3 - counter 5
prenext len 3 - counter 6
prenext len 4 - counter 7
...
...
---next len 125 - counter 250
---next len 126 - counter 251
---next len 126 - counter 252
---next len 127 - counter 253
---next len 127 - counter 254
---next len 128 - counter 255
This time we got half the bars as expected because of the factor 2 requested compression.

The chart:

image

Conclusion
A reconstruction of the market development is possible. Usually a smaller timeframe set of data is available and can be used to discretely replay the timeframe which the system operates on.

The test script.


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import argparse

import backtrader as bt
import backtrader.feeds as btfeeds
import backtrader.indicators as btind


class SMAStrategy(bt.Strategy):
    params = (
        ('period', 10),
        ('onlydaily', False),
    )

    def __init__(self):
        self.sma = btind.SMA(self.data, period=self.p.period)

    def start(self):
        self.counter = 0

    def prenext(self):
        self.counter += 1
        print('prenext len %d - counter %d' % (len(self), self.counter))

    def next(self):
        self.counter += 1
        print('---next len %d - counter %d' % (len(self), self.counter))


def runstrat():
    args = parse_args()

    # Create a cerebro entity
    cerebro = bt.Cerebro(stdstats=False)

    cerebro.addstrategy(
        SMAStrategy,
        # args for the strategy
        period=args.period,
    )

    # Load the Data
    datapath = args.dataname or '../../datas/2006-day-001.txt'
    data = btfeeds.BacktraderCSVData(dataname=datapath)

    # Handy dictionary for the argument timeframe conversion
    tframes = dict(
        daily=bt.TimeFrame.Days,
        weekly=bt.TimeFrame.Weeks,
        monthly=bt.TimeFrame.Months)

    # First add the original data - smaller timeframe
    cerebro.replaydata(data,
                       timeframe=tframes[args.timeframe],
                       compression=args.compression)

    # Run over everything
    cerebro.run()

    # Plot the result
    cerebro.plot(style='bar')


def parse_args():
    parser = argparse.ArgumentParser(
        description='Pandas test script')

    parser.add_argument('--dataname', default='', required=False,
                        help='File Data to Load')

    parser.add_argument('--timeframe', default='weekly', required=False,
                        choices=['daily', 'weekly', 'monhtly'],
                        help='Timeframe to resample to')

    parser.add_argument('--compression', default=1, required=False, type=int,
                        help='Compress n bars into 1')

    parser.add_argument('--period', default=10, required=False, type=int,
                        help='Period to apply to indicator')

    return parser.parse_args()


if __name__ == '__main__':
    runstrat()
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Rollover
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
The RollOver Data Feed
Options for the Roll-Over
Subclassing RollOver
Letâ€™s Roll
Futures concatenation
Futures roll-over with no checks
Changing during the Week
Adding a volume condition
Concluding
Sample Usage
Sample Code
Rolling over Futures
Not every provider offers a continuous future for the instruments with which one can trade. Sometimes the data offered is that of the still valid expiration dates, i.e.: those still being traded

This is not so helpful when it comes to backtesting because the data is scattered over several different instruments which additionally â€¦ overlap in time.

Being able to properly join the data of those instruments, from the past, into a continuous stream alleviates the pain. The problem:

There is no law as to how best join the different expiration dates into a continuous future
Some literature, courtesy of SierraChart at:

http://www.sierrachart.com/index.php?page=doc/ChangingFuturesContract.html
The RollOver Data Feed
backtrader has added with 1.8.10.99 the possibility to join futuresâ€™ data from different expiration dates into a continuous future:


import backtrader as bt

cerebro = bt.Cerebro()
data0 = bt.feeds.MyFeed(dataname='Expiry0')
data1 = bt.feeds.MyFeed(dataname='Expiry1')
...
dataN = bt.feeds.MyFeed(dataname='ExpiryN')

drollover = cerebro.rolloverdata(data0, data1, ..., dataN, name='MyRoll', **kwargs)

cerebro.run()
Note

The possible **kwargs are explained below

It can also be done by directly accessing the RollOver feed (which is helpful if subclassing is done):


import backtrader as bt

cerebro = bt.Cerebro()
data0 = bt.feeds.MyFeed(dataname='Expiry0')
data1 = bt.feeds.MyFeed(dataname='Expiry1')
...
dataN = bt.feeds.MyFeed(dataname='ExpiryN')

drollover = bt.feeds.RollOver(data0, data1, ..., dataN, dataname='MyRoll', **kwargs)
cerebro.adddata(drollover)

cerebro.run()
Note

The possible **kwargs are explained below

Note

When using RollOver the name is assigned using dataname. This is the standard parameter used for all data feeds to pass the name/ticker. In this case it is reused to assign a common name to the complete set of rolled over futures.

In the case of cerebro.rolloverdata, the name is assigned to a feed using name, which is already one named argument of that method

Bottomline:

Data Feeds are created as usual but ARE NOT added to cerebro

Those data feeds are given as input to bt.feeds.RollOver

A dataname is also given, mostly for identification purposes.

This roll over data feed is then added to cerebro

Options for the Roll-Over
Two parameters are provided to control the roll-over process

checkdate (default: None)

This must be a callable with the following signature:


checkdate(dt, d):
Where:

dt is a datetime.datetime object

d is the current data feed for the active future

Expected Return Values:

True: as long as the callable returns this, a switchover can happen to the next future

If a commodity expires on the 3rd Friday of March, checkdate could return True for the entire week in which the expiration takes place.

False: the expiration cannot take place

checkcondition (default: None)

Note

This will only be called if checkdate has returned True

If None this will evaluate to True (execute roll over) internally

Else this must be a callable with this signature:


checkcondition(d0, d1)
Where:

d0 is the current data feed for the active future

d1 is the data feed for the next expiration

Expected Return Values:

True: roll-over to the next future

Following with the example from checkdate, this could say that the roll-over can only happen if the volume from d0 is already less than the volume from d1

False: the expiration cannot take place

Subclassing RollOver
If specifying the callables isnâ€™t enough, there is always the chance to subclass RollOver. The methods to subclass:

def _checkdate(self, dt, d):

Which matches the signature of the parameter of the same name above. The expected return values are also the saame.

def _checkcondition(self, d0, d1)

Which matches the signature of the parameter of the same name above. The expected return values are also the saame.

Letâ€™s Roll
Note

The default behavior in the sample is to use cerebro.rolloverdata. This can be changed by passing the -no-cerebro flag. In this case the sample uses RollOver and cerebro.adddata

The implementation includes a sample which is available in the backtrader sources.

Futures concatenation
Letâ€™s start by looking at a pure concatenation by running the sample with no arguments.


$ ./rollover.py

Len, Name, RollName, Datetime, WeekDay, Open, High, Low, Close, Volume, OpenInterest
0001, FESX, 199FESXM4, 2013-09-26, Thu, 2829.0, 2843.0, 2829.0, 2843.0, 3.0, 1000.0
0002, FESX, 199FESXM4, 2013-09-27, Fri, 2842.0, 2842.0, 2832.0, 2841.0, 16.0, 1101.0
...
0176, FESX, 199FESXM4, 2014-06-20, Fri, 3315.0, 3324.0, 3307.0, 3322.0, 134777.0, 520978.0
0177, FESX, 199FESXU4, 2014-06-23, Mon, 3301.0, 3305.0, 3265.0, 3285.0, 730211.0, 3003692.0
...
0241, FESX, 199FESXU4, 2014-09-19, Fri, 3287.0, 3308.0, 3286.0, 3294.0, 144692.0, 566249.0
0242, FESX, 199FESXZ4, 2014-09-22, Mon, 3248.0, 3263.0, 3231.0, 3240.0, 582077.0, 2976624.0
...
0306, FESX, 199FESXZ4, 2014-12-19, Fri, 3196.0, 3202.0, 3131.0, 3132.0, 226415.0, 677924.0
0307, FESX, 199FESXH5, 2014-12-22, Mon, 3151.0, 3177.0, 3139.0, 3168.0, 547095.0, 2952769.0
...
0366, FESX, 199FESXH5, 2015-03-20, Fri, 3680.0, 3698.0, 3672.0, 3695.0, 147632.0, 887205.0
0367, FESX, 199FESXM5, 2015-03-23, Mon, 3654.0, 3655.0, 3608.0, 3618.0, 802344.0, 3521988.0
...
0426, FESX, 199FESXM5, 2015-06-18, Thu, 3398.0, 3540.0, 3373.0, 3465.0, 1173246.0, 811805.0
0427, FESX, 199FESXM5, 2015-06-19, Fri, 3443.0, 3499.0, 3440.0, 3488.0, 104096.0, 516792.0
This uses cerebro.chaindata and the result should be clear:

Whenever a data feed is over the next one takes over

This happens always between a Friday and Monday: the futures in the samples always expire on Friday

Futures roll-over with no checks
Letâ€™s execute with --rollover


$ ./rollover.py --rollover --plot

Len, Name, RollName, Datetime, WeekDay, Open, High, Low, Close, Volume, OpenInterest
0001, FESX, 199FESXM4, 2013-09-26, Thu, 2829.0, 2843.0, 2829.0, 2843.0, 3.0, 1000.0
0002, FESX, 199FESXM4, 2013-09-27, Fri, 2842.0, 2842.0, 2832.0, 2841.0, 16.0, 1101.0
...
0176, FESX, 199FESXM4, 2014-06-20, Fri, 3315.0, 3324.0, 3307.0, 3322.0, 134777.0, 520978.0
0177, FESX, 199FESXU4, 2014-06-23, Mon, 3301.0, 3305.0, 3265.0, 3285.0, 730211.0, 3003692.0
...
0241, FESX, 199FESXU4, 2014-09-19, Fri, 3287.0, 3308.0, 3286.0, 3294.0, 144692.0, 566249.0
0242, FESX, 199FESXZ4, 2014-09-22, Mon, 3248.0, 3263.0, 3231.0, 3240.0, 582077.0, 2976624.0
...
0306, FESX, 199FESXZ4, 2014-12-19, Fri, 3196.0, 3202.0, 3131.0, 3132.0, 226415.0, 677924.0
0307, FESX, 199FESXH5, 2014-12-22, Mon, 3151.0, 3177.0, 3139.0, 3168.0, 547095.0, 2952769.0
...
0366, FESX, 199FESXH5, 2015-03-20, Fri, 3680.0, 3698.0, 3672.0, 3695.0, 147632.0, 887205.0
0367, FESX, 199FESXM5, 2015-03-23, Mon, 3654.0, 3655.0, 3608.0, 3618.0, 802344.0, 3521988.0
...
0426, FESX, 199FESXM5, 2015-06-18, Thu, 3398.0, 3540.0, 3373.0, 3465.0, 1173246.0, 811805.0
0427, FESX, 199FESXM5, 2015-06-19, Fri, 3443.0, 3499.0, 3440.0, 3488.0, 104096.0, 516792.0
The same behavior. It can clearly be seen that contract changes are being made on the 3rd Friday of either Mar, Jun, Sep, Dec.

But this is mostly WRONG. backtrader cannot know it, but the author knows that the EuroStoxx 50 futures stop trading at 12:00 CET. So even if there is a daily bar for the 3rd Friday of the expiration month, the change is happening too late.

image

Changing during the Week
A checkdate callable is implemented in the sample, which calculates the date of expiration for the currently active contract.

checkdate will allow a roll over as soon as the week of the 3rd Friday of the month is reached (it may be Tuesday if for example Monday is a bank holiday)


$ ./rollover.py --rollover --checkdate --plot

Len, Name, RollName, Datetime, WeekDay, Open, High, Low, Close, Volume, OpenInterest
0001, FESX, 199FESXM4, 2013-09-26, Thu, 2829.0, 2843.0, 2829.0, 2843.0, 3.0, 1000.0
0002, FESX, 199FESXM4, 2013-09-27, Fri, 2842.0, 2842.0, 2832.0, 2841.0, 16.0, 1101.0
...
0171, FESX, 199FESXM4, 2014-06-13, Fri, 3283.0, 3292.0, 3253.0, 3276.0, 734907.0, 2715357.0
0172, FESX, 199FESXU4, 2014-06-16, Mon, 3261.0, 3275.0, 3252.0, 3262.0, 180608.0, 844486.0
...
0236, FESX, 199FESXU4, 2014-09-12, Fri, 3245.0, 3247.0, 3220.0, 3232.0, 650314.0, 2726874.0
0237, FESX, 199FESXZ4, 2014-09-15, Mon, 3209.0, 3224.0, 3203.0, 3221.0, 153448.0, 983793.0
...
0301, FESX, 199FESXZ4, 2014-12-12, Fri, 3127.0, 3143.0, 3038.0, 3042.0, 1409834.0, 2934179.0
0302, FESX, 199FESXH5, 2014-12-15, Mon, 3041.0, 3089.0, 2963.0, 2980.0, 329896.0, 904053.0
...
0361, FESX, 199FESXH5, 2015-03-13, Fri, 3657.0, 3680.0, 3627.0, 3670.0, 867678.0, 3499116.0
0362, FESX, 199FESXM5, 2015-03-16, Mon, 3594.0, 3641.0, 3588.0, 3629.0, 250445.0, 1056099.0
...
0426, FESX, 199FESXM5, 2015-06-18, Thu, 3398.0, 3540.0, 3373.0, 3465.0, 1173246.0, 811805.0
0427, FESX, 199FESXM5, 2015-06-19, Fri, 3443.0, 3499.0, 3440.0, 3488.0, 104096.0, 516792.0
Much better. The roll over is now happening 5 days before. A quick visual inspection of the Len indices show it. For example:

199FESXM4 to 199FESXU4 happens at len 171-172. Without checkdate it happened at 176-177
The roll over is happening on the Monday before the 3rd Friday of the expiration month.

image

Adding a volume condition
Even with the improvement, the situation can be further improved in that not only the date but also de negotiated volume will be taken into account. Do switch when the new contract trades more volume than the currently active one.

Letâ€™s add a checkcondition to the mix and run.


$ ./rollover.py --rollover --checkdate --checkcondition --plot

Len, Name, RollName, Datetime, WeekDay, Open, High, Low, Close, Volume, OpenInterest
0001, FESX, 199FESXM4, 2013-09-26, Thu, 2829.0, 2843.0, 2829.0, 2843.0, 3.0, 1000.0
0002, FESX, 199FESXM4, 2013-09-27, Fri, 2842.0, 2842.0, 2832.0, 2841.0, 16.0, 1101.0
...
0175, FESX, 199FESXM4, 2014-06-19, Thu, 3307.0, 3330.0, 3300.0, 3321.0, 717979.0, 759122.0
0176, FESX, 199FESXU4, 2014-06-20, Fri, 3309.0, 3318.0, 3290.0, 3298.0, 711627.0, 2957641.0
...
0240, FESX, 199FESXU4, 2014-09-18, Thu, 3249.0, 3275.0, 3243.0, 3270.0, 846600.0, 803202.0
0241, FESX, 199FESXZ4, 2014-09-19, Fri, 3273.0, 3293.0, 3250.0, 3252.0, 1042294.0, 3021305.0
...
0305, FESX, 199FESXZ4, 2014-12-18, Thu, 3095.0, 3175.0, 3085.0, 3172.0, 1309574.0, 889112.0
0306, FESX, 199FESXH5, 2014-12-19, Fri, 3195.0, 3200.0, 3106.0, 3147.0, 1329040.0, 2964538.0
...
0365, FESX, 199FESXH5, 2015-03-19, Thu, 3661.0, 3691.0, 3646.0, 3668.0, 1271122.0, 1054639.0
0366, FESX, 199FESXM5, 2015-03-20, Fri, 3607.0, 3664.0, 3595.0, 3646.0, 1182235.0, 3407004.0
...
0426, FESX, 199FESXM5, 2015-06-18, Thu, 3398.0, 3540.0, 3373.0, 3465.0, 1173246.0, 811805.0
0427, FESX, 199FESXM5, 2015-06-19, Fri, 3443.0, 3499.0, 3440.0, 3488.0, 104096.0, 516792.0
Even better. We have moved the switch date to the Thursday before the well known 3rd Friday of the expiration month

This should come to no surprise because the expiring future trades a lot less hours on that Friday and the volume must be small.

Note

The roll over date could have also been set to that Thursday by the checkdate callable. But that isnâ€™t the point of the sample.

image

Concluding
backtrader includes now a flexible mechanism to allow rolling over futures to create a continuous stream.

Sample Usage

$ ./rollover.py --help
usage: rollover.py [-h] [--no-cerebro] [--rollover] [--checkdate]
                   [--checkcondition] [--plot [kwargs]]

Sample for Roll Over of Futures

optional arguments:
  -h, --help            show this help message and exit
  --no-cerebro          Use RollOver Directly (default: False)
  --rollover
  --checkdate           Change during expiration week (default: False)
  --checkcondition      Change when a given condition is met (default: False)
  --plot [kwargs], -p [kwargs]
                        Plot the read data applying any kwargs passed For
                        example: --plot style="candle" (to plot candles)
                        (default: None)
Sample Code

from __future__ import (absolute_import, division, print_function,
                        unicode_literals)


import argparse
import bisect
import calendar
import datetime

import backtrader as bt


class TheStrategy(bt.Strategy):
    def start(self):
        header = ['Len', 'Name', 'RollName', 'Datetime', 'WeekDay', 'Open',
                  'High', 'Low', 'Close', 'Volume', 'OpenInterest']
        print(', '.join(header))

    def next(self):
        txt = list()
        txt.append('%04d' % len(self.data0))
        txt.append('{}'.format(self.data0._dataname))
        # Internal knowledge ... current expiration in use is in _d
        txt.append('{}'.format(self.data0._d._dataname))
        txt.append('{}'.format(self.data.datetime.date()))
        txt.append('{}'.format(self.data.datetime.date().strftime('%a')))
        txt.append('{}'.format(self.data.open[0]))
        txt.append('{}'.format(self.data.high[0]))
        txt.append('{}'.format(self.data.low[0]))
        txt.append('{}'.format(self.data.close[0]))
        txt.append('{}'.format(self.data.volume[0]))
        txt.append('{}'.format(self.data.openinterest[0]))
        print(', '.join(txt))


def checkdate(dt, d):
    # Check if the date is in the week where the 3rd friday of Mar/Jun/Sep/Dec

    # EuroStoxx50 expiry codes: MY
    # M -> H, M, U, Z (Mar, Jun, Sep, Dec)
    # Y -> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 -> year code. 5 -> 2015
    MONTHS = dict(H=3, M=6, U=9, Z=12)

    M = MONTHS[d._dataname[-2]]

    centuria, year = divmod(dt.year, 10)
    decade = centuria * 10

    YCode = int(d._dataname[-1])
    Y = decade + YCode
    if Y < dt.year:  # Example: year 2019 ... YCode is 0 for 2020
        Y += 10

    exp_day = 21 - (calendar.weekday(Y, M, 1) + 2) % 7
    exp_dt = datetime.datetime(Y, M, exp_day)

    # Get the year, week numbers
    exp_year, exp_week, _ = exp_dt.isocalendar()
    dt_year, dt_week, _ = dt.isocalendar()

    # print('dt {} vs {} exp_dt'.format(dt, exp_dt))
    # print('dt_week {} vs {} exp_week'.format(dt_week, exp_week))

    # can switch if in same week
    return (dt_year, dt_week) == (exp_year, exp_week)


def checkvolume(d0, d1):
    return d0.volume[0] < d1.volume[0]  # Switch if volume from d0 < d1


def runstrat(args=None):
    args = parse_args(args)

    cerebro = bt.Cerebro()

    fcodes = ['199FESXM4', '199FESXU4', '199FESXZ4', '199FESXH5', '199FESXM5']
    store = bt.stores.VChartFile()
    ffeeds = [store.getdata(dataname=x) for x in fcodes]

    rollkwargs = dict()
    if args.checkdate:
        rollkwargs['checkdate'] = checkdate

        if args.checkcondition:
            rollkwargs['checkcondition'] = checkvolume

    if not args.no_cerebro:
        if args.rollover:
            cerebro.rolloverdata(name='FESX', *ffeeds, **rollkwargs)
        else:
            cerebro.chaindata(name='FESX', *ffeeds)
    else:
        drollover = bt.feeds.RollOver(*ffeeds, dataname='FESX', **rollkwargs)
        cerebro.adddata(drollover)

    cerebro.addstrategy(TheStrategy)
    cerebro.run(stdstats=False)

    if args.plot:
        pkwargs = dict(style='bar')
        if args.plot is not True:  # evals to True but is not True
            npkwargs = eval('dict(' + args.plot + ')')  # args were passed
            pkwargs.update(npkwargs)

        cerebro.plot(**pkwargs)


def parse_args(pargs=None):

    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        description='Sample for Roll Over of Futures')

    parser.add_argument('--no-cerebro', required=False, action='store_true',
                        help='Use RollOver Directly')

    parser.add_argument('--rollover', required=False, action='store_true')

    parser.add_argument('--checkdate', required=False, action='store_true',
                        help='Change during expiration week')

    parser.add_argument('--checkcondition', required=False,
                        action='store_true',
                        help='Change when a given condition is met')

    # Plot options
    parser.add_argument('--plot', '-p', nargs='?', required=False,
                        metavar='kwargs', const=True,
                        help=('Plot the read data applying any kwargs passed\n'
                              '\n'
                              'For example:\n'
                              '\n'
                              '  --plot style="candle" (to plot candles)\n'))

    if pargs is not None:
        return parser.parse_args(pargs)

    return parser.parse_args()


if __name__ == '__main__':
    runstrat()
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Filters
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Filters
Filters - Reference
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Purpose
Filters at work
Filter Interface
A Sample Filter
Data Pseudo-API for Filters
Another example: Pinkfish Filter
Filters
This functionality is a relatively late addition to backtrader and had to be fitted to the already existing internals. This makes it to be not as flexible and 100% feature full as wished, but it can still serve the purpose in many cases.

Although the implementation tried to allow plug and play filter chaining, the pre-existing internals made it difficult to ensure that could always be achieved. As such, some filters may be chained and some others may not.

Purpose
Transform the values provided by a data feed to deliver a different data feed
The implementation was started to simplify the implementation of the two obvious filters which can be directly used via the cerebro API. These are:

Resampling (cerebro.resampledata)

Here the filter transforms the timeframe and compression of the incoming data feed. For example:


(Seconds, 1) -> (Days, 1)
That means that the original data feed is delivery bars with a resolution of 1 Second. The Resampling filter intercepts the data and buffers it until it can deliver a 1 Day bar. This will happen when a 1 Second bar from the next day is seen.

Replaying (cerebro.replaydata)

For the same timeframes as above, the filter would use the 1 Second resolution bars to rebuild the 1 Day bar.

That means that the 1 Day bar is delivered as many times as 1 Second bars are seen, updated to contain the latest information.

This simulates, for example, how an actual trading day has developed.

Note

The length of the data, len(data) and therefore the length of the strategy remain unchanged as long as the day doesnâ€™t change.

Filters at work
Given an existing data feed/source you use the addfilter method of the data feed:


data = MyDataFeed(dataname=myname)
data.addfilter(filter, *args, **kwargs)
cerebro.addata(data)
And even if it happens to be compatible to the resample/replay filter the following can also be done:


data = MyDataFeed(dataname=myname)
data.addfilter(filter, *args, **kwargs)
cerebro.replaydata(data)
Filter Interface
A filter must conform to a given interface, being this:

A callable which accepts this signature:


callable(data, *args, **kwargs)
or

A class which can be instantiated and called

During instantiation the __init__ method must support the signature:

def __init__(self, data, *args, **kwargs)
The __call__ method bears this signature:

def __call__(self, data, *args, **kwargs)
The instance will be called for each new incoming values from the data feed. The \*args and \*kwargs are the same passed to __init__

RETURN VALUES:


* `True`: the inner data fetching loop of the data feed must retry
  fetching data from the feed, becaue the length of the stream was
  manipulated

* `False` even if data may have been edited (example: changed
  `close` price), the length of the stream has remain untouched
In the case of a class based filter 2 additional methods can be implemented

last with the following signature:

def last(self, data, *args, **kwargs)
This will be called when the data feed is over, allowing the filter to deliver data it may have for example buffered. A typical case is resampling, because a bar is buffered until data from the next time period is seen. When the data feed is over, there is no new data to push the buffered data out.

last offers the chance to push the buffered data out.

Note

It is obvious that if the filter supports no arguments at all and will be added without any, the signatures can be simplified as in:


def __init__(self, data, *args, **kwargs) -> def __init__(self, data)
A Sample Filter
A very quick filter implementation:


class SessionFilter(object):
    def __init__(self, data):
        pass

    def __call__(self, data):
        if data.p.sessionstart <= data.datetime.time() <= data.p.sessionend:
            # bar is in the session
            return False  # tell outer data loop the bar can be processed

        # bar outside of the regular session times
        data.backwards()  # remove bar from data stack
        return True  # tell outer data loop to fetch a new bar
This filter:

Uses data.p.sessionstart and data.p.sessionend (standard data feed parameters) to decide if a bar is in the session.

If in-the-session the return value is False to indicate nothing was done and the processing of the current bar can continue

If not-in-the-session, the bar is removed from the stream and True is returned to indicate a new bar must be fetched.

Note

The data.backwards() makes uses of the LineBuffer interface. This digs deep into the internals of backtrader.

The use of this filter:

Some data feeds contain out of regular trading hours data, which may not be of interest to the trader. With this filter only in-session bars will be considered.
Data Pseudo-API for Filters
In the example above it has been shown how the filter invokes data.backwards() to remove the current bar from the stream. Useful calls from the data feed objects which are meant as a pseudo-API for Filters are:

data.backwards(size=1, force=False): removes size bars from the data stream (default is 1) by moving the logical pointer backwards. If force=True, then the physical storage is also removed.

Removing the physical storage is a delicate operation and is only meant as a hack for internal operations.

data.forward(value=float('NaN'), size=1): moves size bars the storage forward, increasing the physical storage if needed be and fills with value

data._addtostack(bar, stash=False): adds bar to a stack for later processing. bar is an iterable containing as many values as lines has the data feed.

If stash=False the bar added to the stack will be consumed immediately by the system at the beginning of the next iteration.

If stash=True the bar will undergo the entire loop processing including potentially being reparsed by filters

data._save2stack(erase=False, force=False): saves the current data bar to the stack for later processing. If erase=True then data.backwards will be invoked and will receive the parameter force

data._updatebar(bar, forward=False, ago=0): uses the values in the iterable bar to overwrite the values in the data stream ago positions. With the default ago=0 the current bar will updated. With -1, the previous one.

Another example: Pinkfish Filter
This is an example of a filter that can be chained, and is meant so, to another filter, namely the replay filter. The Pinkfish name is from the library which describes the idea in its main page: using daily data to execute operations which would only be possible with intraday data.

To achieve the effect:

A daily bar will be broken in 2 componentes: OHL and then C.

Those 2 pieces are chained with replay to have the following happening in the stream:


With Len X     -> OHL
With Len X     -> OHLC
With Len X + 1 -> OHL
With Len X + 1 -> OHLC
With Len X + 2 -> OHL
With Len X + 2 -> OHLC
...
Logic:

When an OHLC bar is received it is copied into an interable and broken down to become:

An OHL bar. Because this concept doesnâ€™t actually exist the closing price is replaced with the opening price to really form an OHLO bar.

An C bar whic also doesnâ€™t exist. The reality is that it will be delivered like a tick CCCC

The volume if distributed between the 2 parts

The current bar is removed from the stream

The OHLO part is put onto the stack for immediate processing

The CCCC part is put into the stash for processing in the next round

Because the stack has something for immediate processing the filter can return False to indicate it.

This filter works together with:

The replay filter which puts together the OHLO and CCCC parts to finally deliver an OHLC bar.
The use case:

Seeing something like if the maximum today is the highest maximum in the last 20 sessions an issuing a Close order which gets executed with the 2nd tick.
The code:


class DaySplitter_Close(bt.with_metaclass(bt.MetaParams, object)):
    '''
    Splits a daily bar in two parts simulating 2 ticks which will be used to
    replay the data:

      - First tick: ``OHLX``

        The ``Close`` will be replaced by the *average* of ``Open``, ``High``
        and ``Low``

        The session opening time is used for this tick

      and

      - Second tick: ``CCCC``

        The ``Close`` price will be used for the four components of the price

        The session closing time is used for this tick

    The volume will be split amongst the 2 ticks using the parameters:

      - ``closevol`` (default: ``0.5``) The value indicate which percentage, in
        absolute terms from 0.0 to 1.0, has to be assigned to the *closing*
        tick. The rest will be assigned to the ``OHLX`` tick.

    **This filter is meant to be used together with** ``cerebro.replaydata``

    '''
    params = (
        ('closevol', 0.5),  # 0 -> 1 amount of volume to keep for close
    )

    # replaying = True

    def __init__(self, data):
        self.lastdt = None

    def __call__(self, data):
        # Make a copy of the new bar and remove it from stream
        datadt = data.datetime.date()  # keep the date

        if self.lastdt == datadt:
            return False  # skip bars that come again in the filter

        self.lastdt = datadt  # keep ref to last seen bar

        # Make a copy of current data for ohlbar
        ohlbar = [data.lines[i][0] for i in range(data.size())]
        closebar = ohlbar[:]  # Make a copy for the close

        # replace close price with o-h-l average
        ohlprice = ohlbar[data.Open] + ohlbar[data.High] + ohlbar[data.Low]
        ohlbar[data.Close] = ohlprice / 3.0

        vol = ohlbar[data.Volume]  # adjust volume
        ohlbar[data.Volume] = vohl = int(vol * (1.0 - self.p.closevol))

        oi = ohlbar[data.OpenInterest]  # adjust open interst
        ohlbar[data.OpenInterest] = 0

        # Adjust times
        dt = datetime.datetime.combine(datadt, data.p.sessionstart)
        ohlbar[data.DateTime] = data.date2num(dt)

        # Ajust closebar to generate a single tick -> close price
        closebar[data.Open] = cprice = closebar[data.Close]
        closebar[data.High] = cprice
        closebar[data.Low] = cprice
        closebar[data.Volume] = vol - vohl
        ohlbar[data.OpenInterest] = oi

        # Adjust times
        dt = datetime.datetime.combine(datadt, data.p.sessionend)
        closebar[data.DateTime] = data.date2num(dt)

        # Update stream
        data.backwards(force=True)  # remove the copied bar from stream
        data._add2stack(ohlbar)  # add ohlbar to stack
        # Add 2nd part to stash to delay processing to next round
        data._add2stack(closebar, stash=True)

        return False  # initial tick can be further processed from stack
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Filters - Reference
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Filters
Filters - Reference
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
SessionFilter
class backtrader.filters.SessionFilter(data)
SessionFilterSimple
class backtrader.filters.SessionFilterSimple(data)
SessionFilller
class backtrader.filters.SessionFiller(data)
CalendarDays
class backtrader.filters.CalendarDays(data)
BarReplayer_Open
class backtrader.filters.BarReplayer_Open(data)
DaySplitter_Close
class backtrader.filters.DaySplitter_Close(data)
HeikinAshi
class backtrader.filters.HeikinAshi(data)
Renko
class backtrader.filters.Renko(data)
Filters Reference
SessionFilter
class backtrader.filters.SessionFilter(data)
This class can be applied to a data source as a filter and will filter out intraday bars which fall outside of the regular session times (ie: pre/post market data)

This is a â€œnon-simpleâ€ filter and must manage the stack of the data (passed during init and call)

It needs no â€œlastâ€ method because it has nothing to deliver

SessionFilterSimple
class backtrader.filters.SessionFilterSimple(data)
This class can be applied to a data source as a filter and will filter out intraday bars which fall outside of the regular session times (ie: pre/post market data)

This is a â€œsimpleâ€ filter and must NOT manage the stack of the data (passed during init and call)

It needs no â€œlastâ€ method because it has nothing to deliver

Bar Management will be done by the SimpleFilterWrapper class made which is added durint the DataBase.addfilter_simple call

SessionFilller
class backtrader.filters.SessionFiller(data)
Bar Filler for a Data Source inside the declared session start/end times.

The fill bars are constructed using the declared Data Source timeframe and compression (used to calculate the intervening missing times)

Params:

fill_price (def: None):

If None is passed, the closing price of the previous bar will be used. To end up with a bar which for example takes time but it is not displayed in a plot â€¦ use float(â€˜Nanâ€™)

fill_vol (def: float(â€˜NaNâ€™)):

Value to use to fill the missing volume

fill_oi (def: float(â€˜NaNâ€™)):

Value to use to fill the missing Open Interest

skip_first_fill (def: True):

Upon seeing the 1st valid bar do not fill from the sessionstart up to that bar

CalendarDays
class backtrader.filters.CalendarDays(data)
Bar Filler to add missing calendar days to trading days

Params:

fill_price (def: None):

0: The given value to fill 0 or None: Use the last known closing price -1: Use the midpoint of the last bar (High-Low average)

fill_vol (def: float(â€˜NaNâ€™)):

Value to use to fill the missing volume

fill_oi (def: float(â€˜NaNâ€™)):

Value to use to fill the missing Open Interest

BarReplayer_Open
class backtrader.filters.BarReplayer_Open(data)
This filters splits a bar in two parts:

Open: the opening price of the bar will be used to deliver an initial price bar in which the four components (OHLC) are equal

The volume/openinterest fields are 0 for this initial bar

OHLC: the original bar is delivered complete with the original volume/openinterest

The split simulates a replay without the need to use the replay filter.

DaySplitter_Close
class backtrader.filters.DaySplitter_Close(data)
Splits a daily bar in two parts simulating 2 ticks which will be used to replay the data:

First tick: OHLX

The Close will be replaced by the average of Open, High and Low

The session opening time is used for this tick

and

Second tick: CCCC

The Close price will be used for the four components of the price

The session closing time is used for this tick

The volume will be split amongst the 2 ticks using the parameters:

closevol (default: 0.5) The value indicate which percentage, in absolute terms from 0.0 to 1.0, has to be assigned to the closing tick. The rest will be assigned to the OHLX tick.
This filter is meant to be used together with cerebro.replaydata

HeikinAshi
class backtrader.filters.HeikinAshi(data)
The filter remodels the open, high, low, close to make HeikinAshi candlesticks

See:


* [https://en.wikipedia.org/wiki/Candlestick_chart#Heikin_Ashi_candlesticks](https://en.wikipedia.org/wiki/Candlestick_chart#Heikin_Ashi_candlesticks)

* [http://stockcharts.com/school/doku.php?id=chart_school:chart_analysis:heikin_ashi](http://stockcharts.com/school/doku.php?id=chart_school:chart_analysis:heikin_ashi)
Renko
class backtrader.filters.Renko(data)
Modify the data stream to draw Renko bars (or bricks)

Params:

hilo (default: False) Use high and low instead of close to decide if a new brick is needed

size (default: None) The size to consider for each brick

autosize (default: 20.0) If size is None, this will be used to autocalculate the size of the bricks (simply dividing the current price by the given value)

dynamic (default: False) If True and using autosize, the size of the bricks will be recalculated when moving to a new brick. This will of course eliminate the perfect alignment of Renko bricks.

align (default: 1.0) Factor use to align the price boundaries of the bricks. If the price is for example 3563.25 and align is 10.0, the resulting aligned price will be 3560. The calculation:

3563.25 / 10.0 = 356.325

round it and remove the decimals -> 356

356 * 10.0 -> 3560

See:


* [http://stockcharts.com/school/doku.php?id=chart_school:chart_analysis:renko](http://stockcharts.com/school/doku.php?id=chart_school:chart_analysis:renko)
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Yahoo
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
Using the v7 API/format
Using the legacy API/format
Yahoo Data Feed Notes
In May 2017 Yahoo discontinued the existing API for historical data downloads in csv format.

A new API (here named v7) was quickly standardized and has been implemented.

This also brought a change to the actual CSV download format.

Using the v7 API/format
Starting with version 1.9.49.116 this is the default behavior. Choose simply from

YahooFinanceData for online downloads

YahooFinanceCSVData for offline downloaded files

Using the legacy API/format
To use the old API/format

Instantiate the online Yahoo data feed as:


data = bt.feeds.YahooFinanceData(
    ...
    version='',
    ...
)
of the offline Yahoo data feed as:


data = bt.feeds.YahooFinanceCSVData(
    ...
    version='',
    ...
)
It might be that the online service comes back (the service was discontinued without any announcement â€¦ it might as well come back)

or

Only for Offline files downloaded before the change happened, the following can also be done:


data = bt.feeds.YahooLegacyCSV(
    ...
    ...
)
The new YahooLegacyCSV simply automates using version=''

(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Panda
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Pandas DataFeed Example
Note

pandas and its dependencies have to be installed

Supporting Pandas Dataframes seems to be of concern to lots of people, who rely on the already available parsing code for different data sources (including CSV) and other functionalities offered by Pandas.

The important declarations for the Datafeed.

Note

These are ONLY declarations. Don't copy this code blindly. See the actual usage in the example below


class PandasData(feed.DataBase):
    '''
    The ``dataname`` parameter inherited from ``feed.DataBase`` is the pandas
    DataFrame
    '''

    params = (
        # Possible values for datetime (must always be present)
        #  None : datetime is the "index" in the Pandas Dataframe
        #  -1 : autodetect position or case-wise equal name
        #  >= 0 : numeric index to the colum in the pandas dataframe
        #  string : column name (as index) in the pandas dataframe
        ('datetime', None),

        # Possible values below:
        #  None : column not present
        #  -1 : autodetect position or case-wise equal name
        #  >= 0 : numeric index to the colum in the pandas dataframe
        #  string : column name (as index) in the pandas dataframe
        ('open', -1),
        ('high', -1),
        ('low', -1),
        ('close', -1),
        ('volume', -1),
        ('openinterest', -1),
    )
The above excerpt from the PandasData class shows the keys:

The dataname parameter to the class during instantiation holds the Pandas Dataframe

This parameter is inherited from the base class feed.DataBase

The new parameters have the names of the regular fields in the DataSeries and follow these conventions

datetime (default: None)

None : datetime is the â€œindexâ€ in the Pandas Dataframe

-1 : autodetect position or case-wise equal name

= 0 : numeric index to the colum in the pandas dataframe

string : column name (as index) in the pandas dataframe

open, high, low, high, close, volume, openinterest (default: -1 for all of them)

None : column not present

-1 : autodetect position or case-wise equal name

= 0 : numeric index to the colum in the pandas dataframe

string : column name (as index) in the pandas dataframe

A small sample should be able to load the standar 2006 sample, having been parsed by Pandas, rather than directly by backtrader

Running the sample to use the exiting â€œheadersâ€ in the CSV data:


$ ./panda-test.py
--------------------------------------------------
               Open     High      Low    Close  Volume  OpenInterest
Date
2006-01-02  3578.73  3605.95  3578.73  3604.33       0             0
2006-01-03  3604.08  3638.42  3601.84  3614.34       0             0
2006-01-04  3615.23  3652.46  3615.23  3652.46       0             0
The same but telling the script to skip the headers:


$ ./panda-test.py --noheaders
--------------------------------------------------
                  1        2        3        4  5  6
0
2006-01-02  3578.73  3605.95  3578.73  3604.33  0  0
2006-01-03  3604.08  3638.42  3601.84  3614.34  0  0
2006-01-04  3615.23  3652.46  3615.23  3652.46  0  0
The 2nd run is using tells pandas.read_csv:

To skip the first input row (skiprows keyword argument set to 1)

Not to look for a headers row (header keyword argument set to None)

The backtrader support for Pandas tries to automatically detect if column names have been used or else numeric indices and acts accordingly, trying to offer a best match.

The following chart is the tribute to success. The Pandas Dataframe has been correctly loaded (in both cases)

image

The sample code for the test.


from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import argparse

import backtrader as bt
import backtrader.feeds as btfeeds

import pandas


def runstrat():
    args = parse_args()

    # Create a cerebro entity
    cerebro = bt.Cerebro(stdstats=False)

    # Add a strategy
    cerebro.addstrategy(bt.Strategy)

    # Get a pandas dataframe
    datapath = ('../../datas/2006-day-001.txt')

    # Simulate the header row isn't there if noheaders requested
    skiprows = 1 if args.noheaders else 0
    header = None if args.noheaders else 0

    dataframe = pandas.read_csv(datapath,
                                skiprows=skiprows,
                                header=header,
                                parse_dates=True,
                                index_col=0)

    if not args.noprint:
        print('--------------------------------------------------')
        print(dataframe)
        print('--------------------------------------------------')

    # Pass it to the backtrader datafeed and add it to the cerebro
    data = bt.feeds.PandasData(dataname=dataframe)

    cerebro.adddata(data)

    # Run over everything
    cerebro.run()

    # Plot the result
    cerebro.plot(style='bar')


def parse_args():
    parser = argparse.ArgumentParser(
        description='Pandas test script')

    parser.add_argument('--noheaders', action='store_true', default=False,
                        required=False,
                        help='Do not use header rows')

    parser.add_argument('--noprint', action='store_true', default=False,
                        help='Print the dataframe')

    return parser.parse_args()


if __name__ == '__main__':
    runstrat()
(C) 2015-2024 Daniel Rodriguez


Skip to content
logo
Backtrader
Data Feeds - Reference
 DRo
 backtrader
19.3k
4.7k
Search
 
Home
Documentation
Articles
Recipes/Resources
Documentation
Introduction
Installation
Quickstart Guide
Concepts
Cerebro
Data Feeds
Data Feeds
Data Feeds - Extending
Data Feeds - Development - CSV
Data Feeds - Development - General
Data Feeds - Multiple Timeframes
Data Feeds - Resample
Data Feeds - Replay
Data Feeds - Rollover
Data Feeds - Filters
Data Feeds - Yahoo
Data Feeds - Panda
Data Feeds - Reference
Strategy
Indicators
Orders
Broker
Commission Schemes
Analyzers
Observers
Sizers
Live Trading
Plotting
Datetime
Automated Running
Table of contents
AbstractDataBase
BacktraderCSVData
CSVDataBase
Chainer
DataClone
DataFiller
DataFilter
GenericCSVData
IBData
InfluxDB
MT4CSVData
OandaData
PandasData
PandasDirectData
Quandl
QuandlCSV
RollOver
SierraChartCSVData
VCData
VChartCSVData
VChartData
VChartFile
YahooFinanceCSVData
YahooFinanceData
YahooLegacyCSV
Data Feeds Reference
AbstractDataBase
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)
BacktraderCSVData
Parses a self-defined CSV Data used for testing.

Specific parameters:

dataname: The filename to parse or a file-like object
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)
CSVDataBase
Base class for classes implementing CSV DataFeeds

The class takes care of opening the file, reading the lines and tokenizing them.

Subclasses do only need to override:

_loadline(tokens)
The return value of _loadline (True/False) will be the return value of _load which has been overriden by this base class

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)
Chainer
Class that chains datas

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)
DataClone
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)
DataFiller
This class will fill gaps in the source data using the following information bits from the underlying data source

timeframe and compression to dimension the output bars

sessionstart and sessionend

If a data feed has missing bars in between 10:31 and 10:34 and the timeframe is minutes, the output will be filled with bars for minutes 10:32 and 10:33 using the closing price of the last bar (10:31)

Bars can be missinga amongst other things because

Params:


* `fill_price` (def: None): if None (or evaluates to False),the
  closing price will be used, else the passed value (which can be
  for example â€˜NaNâ€™ to have a missing bar in terms of evaluation but
  present in terms of time

* `fill_vol` (def: NaN): used to fill the volume of missing bars

* `fill_oi` (def: NaN): used to fill the openinterest of missing bars
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* fill_price (None)

* fill_vol (nan)

* fill_oi (nan)
DataFilter
This class filters out bars from a given data source. In addition to the standard parameters of a DataBase it takes a funcfilter parameter which can be any callable

Logic:

funcfilter will be called with the underlying data source

It can be any callable

Return value True: current data source bar values will used

Return value False: current data source bar values will discarded

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* funcfilter (None)
GenericCSVData
Parses a CSV file according to the order and field presence defined by the parameters

Specific parameters (or specific meaning):

dataname: The filename to parse or a file-like object

The lines parameters (datetime, open, high â€¦) take numeric values

A value of -1 indicates absence of that field in the CSV source

If time is present (parameter time >=0) the source contains separated fields for date and time, which will be combined

nullvalue

Value that will be used if a value which should be there is missing (the CSV field is empty)

dtformat: Format used to parse the datetime CSV field. See the python strptime/strftime documentation for the format.

If a numeric value is specified, it will be interpreted as follows

1: The value is a Unix timestamp of type int representing the number of seconds since Jan 1st, 1970

2: The value is a Unix timestamp of type float

If a callable is passed

it will accept a string and return a datetime.datetime python instance
tmformat: Format used to parse the time CSV field if â€œpresentâ€ (the default for the â€œtimeâ€ CSV field is not to be present)

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* nullvalue (nan)

* dtformat (%Y-%m-%d %H:%M:%S)

* tmformat (%H:%M:%S)

* datetime (0)

* time (-1)

* open (1)

* high (2)

* low (3)

* close (4)

* volume (5)

* openinterest (6)
IBData
Interactive Brokers Data Feed.

Supports the following contract specifications in parameter dataname:

TICKER # Stock type and SMART exchange

TICKER-STK # Stock and SMART exchange

TICKER-STK-EXCHANGE # Stock

TICKER-STK-EXCHANGE-CURRENCY # Stock

TICKER-CFD # CFD and SMART exchange

TICKER-CFD-EXCHANGE # CFD

TICKER-CDF-EXCHANGE-CURRENCY # Stock

TICKER-IND-EXCHANGE # Index

TICKER-IND-EXCHANGE-CURRENCY # Index

TICKER-YYYYMM-EXCHANGE # Future

TICKER-YYYYMM-EXCHANGE-CURRENCY # Future

TICKER-YYYYMM-EXCHANGE-CURRENCY-MULT # Future

TICKER-FUT-EXCHANGE-CURRENCY-YYYYMM-MULT # Future

TICKER-YYYYMM-EXCHANGE-CURRENCY-STRIKE-RIGHT # FOP

TICKER-YYYYMM-EXCHANGE-CURRENCY-STRIKE-RIGHT-MULT # FOP

TICKER-FOP-EXCHANGE-CURRENCY-YYYYMM-STRIKE-RIGHT # FOP

TICKER-FOP-EXCHANGE-CURRENCY-YYYYMM-STRIKE-RIGHT-MULT # FOP

CUR1.CUR2-CASH-IDEALPRO # Forex

TICKER-YYYYMMDD-EXCHANGE-CURRENCY-STRIKE-RIGHT # OPT

TICKER-YYYYMMDD-EXCHANGE-CURRENCY-STRIKE-RIGHT-MULT # OPT

TICKER-OPT-EXCHANGE-CURRENCY-YYYYMMDD-STRIKE-RIGHT # OPT

TICKER-OPT-EXCHANGE-CURRENCY-YYYYMMDD-STRIKE-RIGHT-MULT # OPT

Params:

sectype (default: STK)

Default value to apply as security type if not provided in the dataname specification

exchange (default: SMART)

Default value to apply as exchange if not provided in the dataname specification

currency (default: '')

Default value to apply as currency if not provided in the dataname specification

historical (default: False)

If set to True the data feed will stop after doing the first download of data.

The standard data feed parameters fromdate and todate will be used as reference.

The data feed will make multiple requests if the requested duration is larger than the one allowed by IB given the timeframe/compression chosen for the data.

what (default: None)

If None the default for different assets types will be used for historical data requests:

â€˜BIDâ€™ for CASH assets

â€˜TRADESâ€™ for any other

Check the IB API docs if another value is wished

rtbar (default: False)

If True the 5 Seconds Realtime bars provided by Interactive Brokers will be used as the smalles tick. According to the documentation they correspond to real-time values (once collated and curated by IB)

If False then the RTVolume prices will be used, which are based on receiving ticks. In the case of CASH assets (like for example EUR.JPY) RTVolume will always be used and from it the bid price (industry de-facto standard with IB according to the literature scattered over the Internet)

Even if set to True, if the data is resampled/kept to a timeframe/compression below Seconds/5, no real time bars will be used, because IB doesnâ€™t serve them below that level

qcheck (default: 0.5)

Time in seconds to wake up if no data is received to give a chance to resample/replay packets properly and pass notifications up the chain

backfill_start (default: True)

Perform backfilling at the start. The maximum possible historical data will be fetched in a single request.

backfill (default: True)

Perform backfilling after a disconnection/reconnection cycle. The gap duration will be used to download the smallest possible amount of data

backfill_from (default: None)

An additional data source can be passed to do an initial layer of backfilling. Once the data source is depleted and if requested, backfilling from IB will take place. This is ideally meant to backfill from already stored sources like a file on disk, but not limited to.

latethrough (default: False)

If the data source is resampled/replayed, some ticks may come in too late for the already delivered resampled/replayed bar. If this is True those ticks will bet let through in any case.

Check the Resampler documentation to see who to take those ticks into account.

This can happen especially if timeoffset is set to False in the IBStore instance and the TWS server time is not in sync with that of the local computer

tradename (default: None) Useful for some specific cases like CFD in which prices are offered by one asset and trading happens in a different onel

SPY-STK-SMART-USD -> SP500 ETF (will be specified as dataname)

SPY-CFD-SMART-USD -> which is the corresponding CFD which offers not price tracking but in this case will be the trading asset (specified as tradename)

The default values in the params are the to allow things like \TICKER, to which the parametersectype(default:STK) andexchange(default:SMART`) are applied.

Some assets like AAPL need full specification including currency (default: â€˜â€™) whereas others like TWTR can be simply passed as it is.

AAPL-STK-SMART-USD would be the full specification for dataname

Or else: IBData as IBData(dataname='AAPL', currency='USD') which uses the default values (STK and SMART) and overrides the currency to be USD

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.5)

* calendar (None)

* sectype (STK)

* exchange (SMART)

* currency ()

* rtbar (False)

* historical (False)

* what (None)

* useRTH (False)

* backfill_start (True)

* backfill (True)

* backfill_from (None)

* latethrough (False)

* tradename (None)
InfluxDB
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* host (127.0.0.1)

* port (8086)

* username (None)

* password (None)

* database (None)

* startdate (None)

* high (high_p)

* low (low_p)

* open (open_p)

* close (close_p)

* volume (volume)

* ointerest (oi)
MT4CSVData
Parses a Metatrader4 History center CSV exported file.

Specific parameters (or specific meaning):

dataname: The filename to parse or a file-like object

Uses GenericCSVData and simply modifies the params

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* nullvalue (nan)

* dtformat (%Y.%m.%d)

* tmformat (%H:%M)

* datetime (0)

* time (1)

* open (2)

* high (3)

* low (4)

* close (5)

* volume (6)

* openinterest (-1)
OandaData
Oanda Data Feed.

Params:

qcheck (default: 0.5)

Time in seconds to wake up if no data is received to give a chance to resample/replay packets properly and pass notifications up the chain

historical (default: False)

If set to True the data feed will stop after doing the first download of data.

The standard data feed parameters fromdate and todate will be used as reference.

The data feed will make multiple requests if the requested duration is larger than the one allowed by IB given the timeframe/compression chosen for the data.

backfill_start (default: True)

Perform backfilling at the start. The maximum possible historical data will be fetched in a single request.

backfill (default: True)

Perform backfilling after a disconnection/reconnection cycle. The gap duration will be used to download the smallest possible amount of data

backfill_from (default: None)

An additional data source can be passed to do an initial layer of backfilling. Once the data source is depleted and if requested, backfilling from IB will take place. This is ideally meant to backfill from already stored sources like a file on disk, but not limited to.

bidask (default: True)

If True, then the historical/backfilling requests will request bid/ask prices from the server

If False, then midpoint will be requested

useask (default: False)

If True the ask part of the bidask prices will be used instead of the default use of bid

includeFirst (default: True)

Influence the delivery of the 1st bar of a historical/backfilling request by setting the parameter directly to the Oanda API calls

reconnect (default: True)

Reconnect when network connection is down

reconnections (default: -1)

Number of times to attempt reconnections: -1 means forever

reconntimeout (default: 5.0)

Time in seconds to wait in between reconnection attemps

This data feed supports only this mapping of timeframe and compression, which comply with the definitions in the OANDA API Developerâ€™s Guid:


(TimeFrame.Seconds, 5): 'S5',
(TimeFrame.Seconds, 10): 'S10',
(TimeFrame.Seconds, 15): 'S15',
(TimeFrame.Seconds, 30): 'S30',
(TimeFrame.Minutes, 1): 'M1',
(TimeFrame.Minutes, 2): 'M3',
(TimeFrame.Minutes, 3): 'M3',
(TimeFrame.Minutes, 4): 'M4',
(TimeFrame.Minutes, 5): 'M5',
(TimeFrame.Minutes, 10): 'M10',
(TimeFrame.Minutes, 15): 'M15',
(TimeFrame.Minutes, 30): 'M30',
(TimeFrame.Minutes, 60): 'H1',
(TimeFrame.Minutes, 120): 'H2',
(TimeFrame.Minutes, 180): 'H3',
(TimeFrame.Minutes, 240): 'H4',
(TimeFrame.Minutes, 360): 'H6',
(TimeFrame.Minutes, 480): 'H8',
(TimeFrame.Days, 1): 'D',
(TimeFrame.Weeks, 1): 'W',
(TimeFrame.Months, 1): 'M',
Any other combination will be rejected

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.5)

* calendar (None)

* historical (False)

* backfill_start (True)

* backfill (True)

* backfill_from (None)

* bidask (True)

* useask (False)

* includeFirst (True)

* reconnect (True)

* reconnections (-1)

* reconntimeout (5.0)
PandasData
Uses a Pandas DataFrame as the feed source, using indices into column names (which can be â€œnumericâ€)

This means that all parameters related to lines must have numeric values as indices into the tuples

Params:

nocase (default True) case insensitive match of column names
Note:

The dataname parameter is a Pandas DataFrame

Values possible for datetime

None: the index contains the datetime

-1: no index, autodetect column

= 0 or string: specific colum identifier

For other lines parameters

None: column not present

-1: autodetect

= 0 or string: specific colum identifier

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* nocase (True)

* datetime (None)

* open (-1)

* high (-1)

* low (-1)

* close (-1)

* volume (-1)

* openinterest (-1)
PandasDirectData
Uses a Pandas DataFrame as the feed source, iterating directly over the tuples returned by â€œitertuplesâ€.

This means that all parameters related to lines must have numeric values as indices into the tuples

Note:

The dataname parameter is a Pandas DataFrame

A negative value in any of the parameters for the Data lines indicates itâ€™s not present in the DataFrame it is

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* datetime (0)

* open (1)

* high (2)

* low (3)

* close (4)

* volume (5)

* openinterest (6)
Quandl
Executes a direct download of data from Quandl servers for the given time range.

Specific parameters (or specific meaning):

dataname

The ticker to download (â€˜YHOOâ€™ for example)

baseurl

The server url. Someone might decide to open a Quandl compatible service in the future.

proxies

A dict indicating which proxy to go through for the download as in {â€˜httpâ€™: â€˜http://myproxy.comâ€™} or {â€˜httpâ€™: â€˜http://127.0.0.1:8080â€™}

buffered

If True the entire socket connection wil be buffered locally before parsing starts.

reverse

Quandl returns the value in descending order (newest first). If this is True (the default), the request will tell Quandl to return in ascending (oldest to newest) format

adjclose

Whether to use the dividend/split adjusted close and adjust all values according to it.

apikey

apikey identification in case it may be needed

dataset

string identifying the dataset to query. Defaults to WIKI

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* reverse (True)

* adjclose (True)

* round (False)

* decimals (2)

* baseurl ([https://www.quandl.com/api/v3/datasets](https://www.quandl.com/api/v3/datasets))

* proxies ({})

* buffered (True)

* apikey (None)

* dataset (WIKI)
QuandlCSV
Parses pre-downloaded Quandl CSV Data Feeds (or locally generated if they comply to the Quandl format)

Specific parameters:

dataname: The filename to parse or a file-like object

reverse (default: False)

It is assumed that locally stored files have already been reversed during the download process

adjclose (default: True)

Whether to use the dividend/split adjusted close and adjust all values according to it.

round (default: False)

Whether to round the values to a specific number of decimals after having adjusted the close

decimals (default: 2)

Number of decimals to round to

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* reverse (False)

* adjclose (True)

* round (False)

* decimals (2)
RollOver
Class that rolls over to the next future when a condition is met

Params:

checkdate (default: None)

This must be a callable with the following signature:


checkdate(dt, d):
Where:

dt is a datetime.datetime object

d is the current data feed for the active future

Expected Return Values:

True: as long as the callable returns this, a switchover can happen to the next future
If a commodity expires on the 3rd Friday of March, checkdate could return True for the entire week in which the expiration takes place.


* `False`: the expiration cannot take place
checkcondition (default: None)

Note: This will only be called if checkdate has returned True

If None this will evaluate to True (execute roll over) internally

Else this must be a callable with this signature:


checkcondition(d0, d1)
Where:

d0 is the current data feed for the active future

d1 is the data feed for the next expiration

Expected Return Values:

True: roll-over to the next future
Following with the example from checkdate, this could say that the roll-over can only happend if the volume from d0 is already less than the volume from d1


* `False`: the expiration cannot take place
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* checkdate (None)

* checkcondition (None)
SierraChartCSVData
Parses a SierraChart CSV exported file.

Specific parameters (or specific meaning):

dataname: The filename to parse or a file-like object

Uses GenericCSVData and simply modifies the dateformat (dtformat) to

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* nullvalue (nan)

* dtformat (%Y/%m/%d)

* tmformat (%H:%M:%S)

* datetime (0)

* time (-1)

* open (1)

* high (2)

* low (3)

* close (4)

* volume (5)

* openinterest (6)
VCData
VisualChart Data Feed.

Params:

qcheck (default: 0.5) Default timeout for waking up to let a resampler/replayer that the current bar can be check for due delivery

The value is only used if a resampling/replaying filter has been inserted in the data

historical (default: False) If no todate parameter is supplied (defined in the base class), this will force a historical only download if set to True

If todate is supplied the same effect is achieved

milliseconds (default: True) The bars constructed by Visual Chart have this aspect: HH:MM:59.999000

If this parameter is True a millisecond will be added to this time to make it look like: HH::MM + 1:00.000000

tradename (default: None) Continous futures cannot be traded but are ideal for data tracking. If this parameter is supplied it will be the name of the current future which will be the trading asset. Example:

001ES -> ES-Mini continuous supplied as dataname

ESU16 -> ES-Mini 2016-09. If this is supplied in tradename it will be the trading asset.

usetimezones (default: True) For most markets the time offset information provided by Visual Chart allows for datetime to be converted to market time (backtrader choice for representation)

Some markets are special (096) and need special internal coverage and timezone support to display in the user expected market time.

If this parameter is set to True importing pytz will be attempted to use timezones (default)

Disabling it will remove timezone usage (may help if the load is excesive)

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.5)

* calendar (None)

* historical (False)

* millisecond (True)

* tradename (None)

* usetimezones (True)
VChartCSVData
Parses a VisualChart CSV exported file.

Specific parameters (or specific meaning):

dataname: The filename to parse or a file-like object
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)
VChartData
Support for Visual Chart binary on-disk files for both daily and intradaily formats.

Note:

dataname: to file or open file-like object

If a file-like object is passed, the timeframe parameter will be used to determine which is the actual timeframe.

Else the file extension (.fd for daily and .min for intraday) will be used.

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)
VChartFile
Support for Visual Chart binary on-disk files for both daily and intradaily formats.

Note:

dataname: Market code displayed by Visual Chart. Example: 015ES for EuroStoxx 50 continuous future
Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)
YahooFinanceCSVData
Parses pre-downloaded Yahoo CSV Data Feeds (or locally generated if they comply to the Yahoo format)

Specific parameters:

dataname: The filename to parse or a file-like object

reverse (default: False)

It is assumed that locally stored files have already been reversed during the download process

adjclose (default: True)

Whether to use the dividend/split adjusted close and adjust all values according to it.

adjvolume (default: True)

Do also adjust volume if adjclose is also True

round (default: True)

Whether to round the values to a specific number of decimals after having adjusted the close

roundvolume (default: 0)

Round the resulting volume to the given number of decimals after having adjusted it

decimals (default: 2)

Number of decimals to round to

swapcloses (default: False)

[2018-11-16] It would seem that the order of close and adjusted close is now fixed. The parameter is retained, in case the need to swap the columns again arose.

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime

* adjclose
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* reverse (False)

* adjclose (True)

* adjvolume (True)

* round (True)

* decimals (2)

* roundvolume (False)

* swapcloses (False)
YahooFinanceData
Executes a direct download of data from Yahoo servers for the given time range.

Specific parameters (or specific meaning):

dataname

The ticker to download (â€˜YHOOâ€™ for Yahoo own stock quotes)

proxies

A dict indicating which proxy to go through for the download as in {â€˜httpâ€™: â€˜http://myproxy.comâ€™} or {â€˜httpâ€™: â€˜http://127.0.0.1:8080â€™}

period

The timeframe to download data in. Pass â€˜wâ€™ for weekly and â€˜mâ€™ for monthly.

reverse

[2018-11-16] The latest incarnation of Yahoo online downloads returns the data in the proper order. The default value of reverse for the online download is therefore set to False

adjclose

Whether to use the dividend/split adjusted close and adjust all values according to it.

urlhist

The url of the historical quotes in Yahoo Finance used to gather a crumb authorization cookie for the download

urldown

The url of the actual download server

retries

Number of times (each) to try to get a crumb cookie and download the data

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime

* adjclose
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* reverse (False)

* adjclose (True)

* adjvolume (True)

* round (True)

* decimals (2)

* roundvolume (False)

* swapcloses (False)

* proxies ({})

* period (d)

* urlhist ([https://finance.yahoo.com/quote](https://finance.yahoo.com/quote)/{}/history)

* urldown ([https://query1.finance.yahoo.com/v7/finance/download](https://query1.finance.yahoo.com/v7/finance/download))

* retries (3)
YahooLegacyCSV
This is intended to load files which were downloaded before Yahoo discontinued the original service in May-2017

Lines:


* close

* low

* high

* open

* volume

* openinterest

* datetime

* adjclose
Params:


* dataname (None)

* name ()

* compression (1)

* timeframe (5)

* fromdate (None)

* todate (None)

* sessionstart (None)

* sessionend (None)

* filters ([])

* tz (None)

* tzinput (None)

* qcheck (0.0)

* calendar (None)

* headers (True)

* separator (,)

* reverse (False)

* adjclose (True)

* adjvolume (True)

* round (True)

* decimals (2)

* roundvolume (False)

* swapcloses (False)

* version ()
(C) 2015-2024 Daniel Rodriguez


